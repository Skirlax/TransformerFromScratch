{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "573a8ff2-695d-48d9-9cef-1be350952512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46432f12-ab7d-46d0-ad73-a44fff69742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        ''' attention(q,k,v) = sum_i(similarity(q, k_i)*v_i\n",
    "            where q,k,v are n-dim learned vectors from input embedding\n",
    "            and similarity is the scalar result of q dot k\n",
    "            \n",
    "            Hence, attention is a learned representation of input embedding\n",
    "            x_1 from sentence (or sequence) [x_1, x_2, ..., x_i],\n",
    "            based on weighted average of v_i's, where the scalar weights are \n",
    "            based on similarity of q = W_q*x_i among the rest of the \n",
    "            k_i = W_k*x_i (tokens in the same sentence).\n",
    "        '''\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\" # why?\n",
    "\n",
    "        self.d_model = d_model # d_model is the concat'd dimensionality of the num_head instances of the input_embeddings\n",
    "        self.num_heads = num_heads # num_heads is the # of kinds of weights to be learned for the weigthed averaging operation\n",
    "        self.d_k = d_model // num_heads # d_k is the input embedding dimensionality (vector length)\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model) # y = x*W.t + b, weights to convert input embedding to learned representation Q\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model) # I'm guessing that the purpose of this is to learn a final \n",
    "                                               # combination for the multi-head attentions to produce a single long attention vector\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # What are the shapes of Q,K, and V?\n",
    "        # Q,K,V shape is (batch_size, seq_len, d_model) when combined,\n",
    "        # (batch_size, num_heads, seq_len, d_k) when split into each attention vector\n",
    "        # when they enter this operation, they're SPLIT\n",
    "        \n",
    "        # K's last two dim (seq_len, d_k) are swapped (transposed)\n",
    "        # This matmul computes the similarity of all tokens against each of the other tokens (hence seq_len,seq_len shape)\n",
    "        # It does this per head as well.\n",
    "        # Shape of similarity_scores: (batch_sz, num_heads, seq_len, seq_len)\n",
    "        similarity_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            similarity_scores = similarity_scores.masked_fill(mask == 0, -1e9) # where mask is 0, replace tensor element value with -1e9 (e^x, used by softmax later, is 0 at -inf)\n",
    "         \n",
    "        similarity_probs = torch.softmax(similarity_scores, dim=-1) # apply softmax on last dim (seq_len column, \n",
    "                                                                    # so each row adds to 1, each column represents a \n",
    "                                                                    # token's similarity to token_i=row_num)\n",
    "        # Output shape is back to (batch_sz, num_head, seq_len, d_k) \n",
    "        # Double check how each score is applied to V, it doesn't seem to perform the weighted average operation\n",
    "        # The intuition here follows the \"row interpretation\" of a matrix multiplication\n",
    "        # where matmul can be thought of as: each row of the matrix on the left is a vector of weights for taking a linear combination of the rows of the matrix on the right! \n",
    "        # That is, each row on the left produces its own linear combination of the rows of the matrix on the right, and they all get stacked together row-by-row in the output matrix.\n",
    "        # see: https://forums.fast.ai/t/fun-matrix-math-example-the-transformers-attention-mechanism/41606\n",
    "        output = torch.matmul(similarity_probs, V)\n",
    "        return output\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        # Final shape: (batch_sz, num_heads, seq_len, d_k)\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1,2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        # Return as contiguous chunk in memory\n",
    "        # Final shape: (batch_sz, seq_len, d_model)\n",
    "        return x.transpose(1,2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q)) # Note how the QKV operations are performed per head, separately\n",
    "        K = self.split_heads(self.W_k(K)) # while on W_o, all attentions are then combined to transform into one.\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "                                                                                                                            \n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bacc583-82b0-4e5e-99aa-e49fffd19dfc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "t = MultiHeadAttention(6,2) # d_model=6,num_heads=2 \n",
    "x = torch.tensor([[[1.,2.,3.,4.,5.,6.], [2.,4.,6.,8.,10.,12.]]]) # batch_sz=1, seq_len=2, d_model=6\n",
    "y = t(x, x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2accd70a-decf-46ac-96fd-73f3f0c5542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    ''' To be stacked after MultiHeadAttn. This will take in the\n",
    "        attn vector combined (via add & norm) with the input embedding\n",
    "        via a residual connection (it skipped the MultiHead).\n",
    "        I don't know yet why it is designed this way.\n",
    "        Maybe to retain more info about the input embedding AND POSITIONAL ENCODING\n",
    "        instead of just indirectly working on it via Q, K, and V?\n",
    "    '''\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff) # in_dim=d_model, out_dim=d_ff, where d_ff is a hyperparam for internal dim of feedforward layer\n",
    "        self.fc1 = nn.Linear(d_ff, d_model) # final output shape of feed forward is d_model\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x))) # why no activation on last layer?\n",
    "                                                # range is all real numbers?\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        # d_model = dimension of the concatenated learned embeddings from all heads\n",
    "        # num_heads = number of attention heads that will learn an embedding\n",
    "        # d_ff = feed forward inner dimension of the 2-layer MLP (d_model, d_ff, d_model)\n",
    "        # dropout = probability of dropout applied on output of MultiHeadAttn & of MLP layer\n",
    "        \n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model) # standard normalization with learnable scale & bias\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout) # randomly zero out some input elements with probability=dropout, for regularization\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # mask is a matrix where zero elements will result to masking out some learned embeddings in V\n",
    "        \n",
    "        attn_output = self.dropout(self.self_attn(x, x, x, mask)) # MultiHeadAttn\n",
    "        x = self.norm1(x + attn_output)                           # Add & Norm (w/ residual connx)\n",
    "\n",
    "        ff_output = self.dropout(self.feed_forward(x))            # Feed Forward\n",
    "        x = self.norm2(x + ff_output)                             # Add & Norm (w/ residual connx)\n",
    "\n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.dropout(self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.norm1(x + attn_output)\n",
    "        \n",
    "        attn_output = self.dropout(self.cross_attn(x, enc_output, enc_output, src_mask)) #Q=x, K,V=enc_output\n",
    "        x = self.norm2(x + attn_output)\n",
    "        \n",
    "        ff_output = self.dropout(self.feed_forward(x))  \n",
    "        x = self.norm3(x + ff_output)                  \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3fc9570b-1aea-412f-83e8-5449ea134fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1.]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(1,2,3)\n",
    "a[:,:1,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2332f28a-7ba5-4099-82c7-843ef04e352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len):\n",
    "        # max_seq_len is the max sentence len (num of tokens in a sequence)\n",
    "        super.__init__()\n",
    "\n",
    "        pe = torch.zeros(max_seq_len, d_model) # init PosEnc as 2D matrix of zeros (each row is a token)\n",
    "        # position = [[0],[1], ...]], column vector of positions from 0 to max_seq_len-1\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        # div_term is a freq defined for each dimension of the embedding of len d_model\n",
    "        # skip by two bc this will be fed to both sin and cos (covering whole d_model len)\n",
    "        # div_term is a row vector\n",
    "        exp_term = torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)\n",
    "        div_term = torch.exp(exp_term) # row vector\n",
    "        \n",
    "        # matmul(col_vec,row_vec) to get matrix.shape=(max_seq_len, d_model//2) \n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # broadcast, assign to all tokens and on even dims of embedding\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # all max_seq_len tokens, on odd indices\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # now we have a matrix (1, max_seq_len, d_model) with corresponding \n",
    "        # position value per token (ie row)\n",
    "        \n",
    "        # register_buffer => Tensor which is not a trainable-parameter,\n",
    "        # but should be part of the modules variables.\n",
    "        # persistent=False, to not add the buffer to the state dict (e.g. when we save the model)\n",
    "        self.register_buffer('pe', pe, persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # expects input of shape (batch_sz, max_seq_len, d_model)\n",
    "        return x + self.pe[:, :x.size(1)] # add to all batches, up to seq_len, vectors of len d_model\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_sz, tgt_vocab_sz, d_model, num_heads,\n",
    "                       num_layers, d_ff, max_seq_len, dropout):\n",
    "        super().__init__()\n",
    "        # src_vocab_sz = num of words in src vocab\n",
    "        # tgt_vocab_sz = num of words in tgt vocab\n",
    "        # d_model = resulting len of embedding vector\n",
    "        # max_seq_len = maximum number of words or tokens in a sentence or sequence\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_sz, d_model) # Simple lookup table that returns an embedding vector for given indices\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_sz, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "\n",
    "        # num_layers = number of encoder and decoder layers for embedding to pass through\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout)]*num_layers)\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout)]*num_layers)\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2) # insert dim=1 at specified position\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_len = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_len, seq_len), diagonal = 1)).bool() # get matrix's lower triangle of True's \n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        \n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # src, tgt = a sentence is a batch of vectors containing indices for words (to be embedded)\n",
    "        #          = shape is (batch_sz, max_seq_len)\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embed = self.encoder_embedding(src) # outputs (batch_sz, max_seq_len, d_model)\n",
    "        src_embed = self.dropout(self.positional_encoding(src_embed)) # add positional info\n",
    "        \n",
    "        tgt_embed = self.dropout(self.positional_encoding(self.decoder_embedding(tgt))) # same as above but for tgt embeds\n",
    "\n",
    "        enc_output = src_embed\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embed\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            # NOTE: the same final encoder output is used for all dec layers\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = sef.fc(dec_output)\n",
    "        \n",
    "        return output\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd2f1a35-ad56-4159-97ef-01e10d445667",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Dropout(p=0.2)\n",
    "input = torch.randn(2, 1)\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c063942-ce23-4b4a-bb17-9e5644af9839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5911],\n",
      "        [-0.0098]])\n",
      "tensor([[0.7389],\n",
      "        [-0.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c238f96-40a0-42e4-839b-cf529c76994b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 4\n",
    "nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal = 1)).bool()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "784f74f4-9ffc-4e82-b0f6-1a3dd518075c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False, False],\n",
       "         [ True,  True, False, False],\n",
       "         [ True,  True,  True, False],\n",
       "         [ True,  True,  True,  True]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nopeak_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccb742d-02ce-4103-9b25-5066f5971edf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
