{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "573a8ff2-695d-48d9-9cef-1be350952512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46432f12-ab7d-46d0-ad73-a44fff69742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        ''' attention(q,k,v) = sum_i(similarity(q, k_i)*v_i\n",
    "            where q,k,v are n-dim learned vectors from input embedding\n",
    "            and similarity is the scalar result of q dot k\n",
    "            \n",
    "            Hence, attention is a learned representation of input embedding\n",
    "            x_1 from sentence (or sequence) [x_1, x_2, ..., x_i],\n",
    "            based on weighted average of v_i's, where the scalar weights are \n",
    "            based on similarity of q = W_q*x_i among the rest of the \n",
    "            k_i = W_k*x_i (tokens in the same sentence).\n",
    "        '''\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\" # why?\n",
    "\n",
    "        self.d_model = d_model # d_model is the concat'd dimensionality of the num_head instances of the input_embeddings\n",
    "        self.num_heads = num_heads # num_heads is the # of kinds of weights to be learned for the weigthed averaging operation\n",
    "        self.d_k = d_model // num_heads # d_k is the input embedding dimensionality (vector length)\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model) # y = x*W.t + b, weights to convert input embedding to learned representation Q\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model) # I'm guessing that the purpose of this is to learn a final \n",
    "                                               # combination for the multi-head attentions to produce a single long attention vector\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # What are the shapes of Q,K, and V?\n",
    "        # Q,K,V shape is (batch_size, seq_len, d_model) when combined,\n",
    "        # (batch_size, num_heads, seq_len, d_k) when split into each attention vector\n",
    "        # when they enter this operation, they're SPLIT\n",
    "        \n",
    "        # K's last two dim (seq_len, d_k) are swapped (transposed)\n",
    "        # This matmul computes the similarity of all tokens against each of the other tokens (hence seq_len,seq_len shape)\n",
    "        # It does this per head as well.\n",
    "        # Shape of similarity_scores: (batch_sz, num_heads, seq_len, seq_len)\n",
    "        similarity_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            similarity_scores = similarity_scores.masked_fill(mask == 0, -1e9) # where mask is 0, replace tensor element value with -1e9 (e^x, used by softmax later, is 0 at -inf)\n",
    "         \n",
    "        similarity_probs = torch.softmax(similarity_scores, dim=-1) # apply softmax on last dim (seq_len column, \n",
    "                                                                    # so each row adds to 1, each column represents a \n",
    "                                                                    # token's similarity to token_i=row_num)\n",
    "        # Output shape is back to (batch_sz, num_head, seq_len, d_k) \n",
    "        # Double check how each score is applied to V, it doesn't seem to perform the weighted average operation\n",
    "        # The intuition here follows the \"row interpretation\" of a matrix multiplication\n",
    "        # where matmul can be thought of as: each row of the matrix on the left is a vector of weights for taking a linear combination of the rows of the matrix on the right! \n",
    "        # That is, each row on the left produces its own linear combination of the rows of the matrix on the right, and they all get stacked together row-by-row in the output matrix.\n",
    "        # see: https://forums.fast.ai/t/fun-matrix-math-example-the-transformers-attention-mechanism/41606\n",
    "        output = torch.matmul(similarity_probs, V)\n",
    "        return output\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        # Final shape: (batch_sz, num_heads, seq_len, d_k)\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1,2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        # Return as contiguous chunk in memory\n",
    "        # Final shape: (batch_sz, seq_len, d_model)\n",
    "        return x.transpose(1,2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q)) # Note how the QKV operations are performed per head, separately\n",
    "        K = self.split_heads(self.W_k(K)) # while on W_o, all attentions are then combined to transform into one.\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "                                                                                                                            \n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "\n",
    "        return output # shape is (batch_sz, seq_len, d_model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bacc583-82b0-4e5e-99aa-e49fffd19dfc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "t = MultiHeadAttention(6,2) # d_model=6,num_heads=2 \n",
    "x = torch.tensor([[[1.,2.,3.,4.,5.,6.], [2.,4.,6.,8.,10.,12.]]]) # batch_sz=1, seq_len=2, d_model=6\n",
    "y = t(x, x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2accd70a-decf-46ac-96fd-73f3f0c5542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    ''' To be stacked after MultiHeadAttn. This will take in the\n",
    "        attn vector combined (via add & norm) with the input embedding\n",
    "        via a residual connection (it skipped the MultiHead).\n",
    "        I don't know yet why it is designed this way.\n",
    "        Maybe to retain more info about the input embedding AND POSITIONAL ENCODING\n",
    "        instead of just indirectly working on it via Q, K, and V?\n",
    "    '''\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff) # in_dim=d_model, out_dim=d_ff, where d_ff is a hyperparam for internal dim of feedforward layer\n",
    "        self.fc2 = nn.Linear(d_ff, d_model) # final output shape of feed forward is d_model\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x))) # why no activation on last layer?\n",
    "                                                # range is all real numbers?\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        # d_model = dimension of the concatenated learned embeddings from all heads\n",
    "        # num_heads = number of attention heads that will learn an embedding\n",
    "        # d_ff = feed forward inner dimension of the 2-layer MLP (d_model, d_ff, d_model)\n",
    "        # dropout = probability of dropout applied on output of MultiHeadAttn & of MLP layer\n",
    "        \n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model) # standard normalization with learnable scale & bias\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout) # randomly zero out some input elements with probability=dropout, for regularization\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # mask is a matrix where zero elements will result to masking out some learned embeddings in V\n",
    "        \n",
    "        attn_output = self.dropout(self.self_attn(x, x, x, mask)) # MultiHeadAttn\n",
    "        x = self.norm1(x + attn_output)                           # Add & Norm (w/ residual connx)\n",
    "\n",
    "        ff_output = self.dropout(self.feed_forward(x))            # Feed Forward\n",
    "        x = self.norm2(x + ff_output)                             # Add & Norm (w/ residual connx)\n",
    "\n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.dropout(self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.norm1(x + attn_output)\n",
    "        \n",
    "        attn_output = self.dropout(self.cross_attn(x, enc_output, enc_output, src_mask)) #Q=x, K,V=enc_output\n",
    "        x = self.norm2(x + attn_output)\n",
    "        \n",
    "        ff_output = self.dropout(self.feed_forward(x))  \n",
    "        x = self.norm3(x + ff_output)                  \n",
    "\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2332f28a-7ba5-4099-82c7-843ef04e352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len):\n",
    "        # max_seq_len is the max sentence len (num of tokens in a sequence)\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_seq_len, d_model) # init PosEnc as 2D matrix of zeros (each row is a token)\n",
    "        # position = [[0],[1], ...]], column vector of positions from 0 to max_seq_len-1\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        # div_term is a freq defined for each dimension of the embedding of len d_model\n",
    "        # skip by two bc this will be fed to both sin and cos (covering whole d_model len)\n",
    "        # div_term is a row vector\n",
    "        exp_term = torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)\n",
    "        div_term = torch.exp(exp_term) # this formula is same as in the reference, just more efficient.\n",
    "                                       # just equate the term to e^x, and solve for x, then e^x again. \n",
    "                                       # See: https://ai.stackexchange.com/q/41670\n",
    "        \n",
    "        # matmul(col_vec,row_vec) to get matrix.shape=(max_seq_len, d_model//2) \n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # broadcast, assign to all tokens and on even dims of embedding\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # all max_seq_len tokens, on odd indices\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # now we have a matrix (1, max_seq_len, d_model) with corresponding \n",
    "        # position value per token (ie row)\n",
    "        \n",
    "        # register_buffer => Tensor which is not a trainable-parameter,\n",
    "        # but should be part of the modules variables.\n",
    "        # persistent=False, to not add the buffer to the state dict (e.g. when we save the model)\n",
    "        self.register_buffer('pe', pe, persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # expects input of shape (batch_sz, max_seq_len, d_model)\n",
    "        return x + self.pe[:, :x.size(1)] # add to all batches, up to seq_len, vectors of len d_model\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_sz, tgt_vocab_sz, d_model, num_heads,\n",
    "                       num_layers, d_ff, max_seq_len, dropout):\n",
    "        super().__init__()\n",
    "        # src_vocab_sz = num of words in src vocab\n",
    "        # tgt_vocab_sz = num of words in tgt vocab\n",
    "        # d_model = resulting len of embedding vector\n",
    "        # max_seq_len = maximum number of words or tokens in a sentence or sequence\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_sz, d_model) # Simple lookup table that returns an embedding vector for given indices\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_sz, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "\n",
    "        # num_layers = number of encoder and decoder layers for embedding to pass through\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout)]*num_layers)\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout)]*num_layers)\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2) # insert dim=1 at specified position\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_len = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_len, seq_len), diagonal = 1)).bool() # get matrix's lower triangle of True's \n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        \n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # src, tgt = a sentence is a batch of vectors containing indices for words (to be embedded)\n",
    "        #          = shape is (batch_sz, max_seq_len)\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embed = self.encoder_embedding(src) # outputs (batch_sz, max_seq_len, d_model)\n",
    "        src_embed = self.dropout(self.positional_encoding(src_embed)) # add positional info\n",
    "        \n",
    "        tgt_embed = self.dropout(self.positional_encoding(self.decoder_embedding(tgt))) # same as above but for tgt embeds\n",
    "\n",
    "        enc_output = src_embed\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embed\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            # NOTE: the same final encoder output is used for all dec layers\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = sef.fc(dec_output) # shape is (batch_sz, seq_len, tgt_vocab_size)\n",
    "        \n",
    "        return output # shape (batch_sz, seq_len, d_model)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bd2f1a35-ad56-4159-97ef-01e10d445667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sample data\n",
    "src_vocab_size = 1000\n",
    "tgt_vocab_size = 1000\n",
    "d_model = 128\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "d_ff = 512\n",
    "max_seq_length = 50\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "# Generate random sample data\n",
    "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7c063942-ce23-4b4a-bb17-9e5644af9839",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PositionWiseFeedForward' object has no attribute 'fc2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m): \u001b[38;5;66;03m# no batches, use same data\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 9\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# decoder input is 1 to before end\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     _output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, tgt_vocab_size) \u001b[38;5;66;03m# -1 means derive that size from other dims\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     target \u001b[38;5;241m=\u001b[39m tgt_data[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# shifted sequence by 1 to the right (and flattened)\u001b[39;00m\n",
      "File \u001b[0;32m~/Development/TransformerFromScratch/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Development/TransformerFromScratch/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[38], line 72\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, tgt)\u001b[0m\n\u001b[1;32m     70\u001b[0m enc_output \u001b[38;5;241m=\u001b[39m src_embed\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m enc_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_layers:\n\u001b[0;32m---> 72\u001b[0m     enc_output \u001b[38;5;241m=\u001b[39m \u001b[43menc_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m dec_output \u001b[38;5;241m=\u001b[39m tgt_embed\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dec_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_layers:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# NOTE: the same final encoder output is used for all dec layers\u001b[39;00m\n",
      "File \u001b[0;32m~/Development/TransformerFromScratch/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Development/TransformerFromScratch/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 39\u001b[0m, in \u001b[0;36mEncoderLayer.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     36\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(x, x, x, mask)) \u001b[38;5;66;03m# MultiHeadAttn\u001b[39;00m\n\u001b[1;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m attn_output)                           \u001b[38;5;66;03m# Add & Norm (w/ residual connx)\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m ff_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)            \u001b[38;5;66;03m# Feed Forward\u001b[39;00m\n\u001b[1;32m     40\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m ff_output)                             \u001b[38;5;66;03m# Add & Norm (w/ residual connx)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Development/TransformerFromScratch/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Development/TransformerFromScratch/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 16\u001b[0m, in \u001b[0;36mPositionWiseFeedForward.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc2\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x)))\n",
      "File \u001b[0;32m~/Development/TransformerFromScratch/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PositionWiseFeedForward' object has no attribute 'fc2'"
     ]
    }
   ],
   "source": [
    "# Sample training\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr= 0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(10): # no batches, use same data\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(src_data, tgt_data[:, :-1]) # decoder input is 1 to before end\n",
    "    _output = output.contiguous().view(-1, tgt_vocab_size) # -1 means derive that size from other dims\n",
    "    target = tgt_data[:, 1:].contiguous().view(-1) # shifted sequence by 1 to the right (and flattened)\n",
    "    \n",
    "    loss = criterion(_output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c238f96-40a0-42e4-839b-cf529c76994b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 4\n",
    "nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal = 1)).bool()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "784f74f4-9ffc-4e82-b0f6-1a3dd518075c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5132, -0.7557, -1.5234, -0.4160],\n",
       "        [ 1.7056, -1.4635, -1.1913, -1.3772]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2,2,2)\n",
    "x.view(-1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccb742d-02ce-4103-9b25-5066f5971edf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
